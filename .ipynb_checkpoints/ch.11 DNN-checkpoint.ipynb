{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 2와 파이썬 3 지원\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# 공통\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# 맷플롯립 설정\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 한글출력\n",
    "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 그림을 저장할 폴더\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번장에서는 3가지 주제를 가지고 다루어 볼려고 한다. 이는 깊은 심층 신경망을 훈련시킬때 발생하는 문제들이다.\n",
    "1. 그래디언트 소실 문제를 설명하고 가장 알려진 해결방법을 살펴본다.\n",
    "2. 대규모 모델의 훈련속도를 크게 높여주는 다양한 최적화 방법을 알아본다.\n",
    "3. 과대적합(overfit)를 막기위한 규제(regularization)기법을 알아본다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.그래이던트 소실 폭주 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파 알고리즘은 출력층에서 입력층으로 오차 그래디언트를 전달시킨다. 그러나 알고리즘이 하위층으로 진행됨에 따라서 그래디어트가 점점 작아지는 현상이 나타날때가 있습니다. 결국 경사하강법이 하위층의 연결 가중치를 실제 변경되지 않은 채로 둔다면 훈련이 좋은 솔루션으로 수렴되지 않습니다. 이 문제를 '그래디언트 소실'이라고 한다. 어떤경우는 반대로 일어난다. 그래디언트가 점점 커져 여러 개의 층이 비정상적으로 큰 가중치로 갱신되면 알고리즘은 발산(diverse)하게 된다. 이 문제를 '그래디언트 폭주'라고 하고 주로 순환 신경망(RNN)에서 주로 나타납니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Understanding the Difficulty of Training Deep Feedforward Neural Networks)에서 로지스틱 시그모이드 활성화 함수와 정규분포의 가중치 초기화 방법을 사용할 때 신경망의 위쪽으로 갈수록 층을 지날 때마다 분산이 커져 가장 높은 층에서는 활성화 함수가 0또는 1로 수렴하게 된다는 것이다. 이 때문에 그래디언트가 거의 없는 부분이 되고 실재로 아래층에 아무것도 도달되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\matplotlib\\font_manager.py:1316: UserWarning: findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226a837fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('convergence', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('convergence', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"logistic\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier와 He 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에 논문에서 글로럿과 벤지오는 이 문제를 완화시키는 방법을 제안합니다. 정방향과 역방향 즉 양방향의 신호가 적절하게 흘러야 합니다. 적절한 신호가 흐르기 위해서는 출력에 대한 분산값과 입력에 대한 분산값이 같아야 한다고 이야기합니다. 그리고 그래디언트 분산 또한 동일해야 합니다. 사실 층의 입력과 출력 연결 개수가 같지 않다면 이 두가지를 보장할 수 없습니다. 하지만 이들은 실전에서 매우 잘 작동한다고 입증된 대안을 제안합니다. 연결가중치를 다음과 같이 무작위로 초기화 하는 것 입니다.\n",
    "\n",
    "평균 = 0 , stdev = sqrt(2/(n_input+n_output))인 정규분포 또는 r = sqrt(6/(n_input+n_output))일때 -r과+r사이의 균등분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 다른 활성화 함수에 대해서 활용하는 분포도 존재합니다. 이를 He inialization이라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer() #He 초기화 방식\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수렴하지 않는 활성화 함수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에 논문의 결과에 따라서 수렴하지 않는 활성화 함수를 쓰는 것이 큰 장점을 가지고 있습니다. 그래서 ReLU를 사용했다. 그러나 ReLU함수는 완벽하지 않습니다. dying ReLU현상이 있습니다. 이는 훈련하는 동안 일부뉴런이 0 이외의 값을 출력하지 않는다는 의미입니다. 특히 큰 학습률을 사용하면 신경망의 뉴런 절반이 죽어있기도 합니다. 훈련 도중 뉴런의 가중치가 바뀌어 가중치 합이 음수가 되면 그다음부터 0을 출력하기 시작할 것입니다. ReLU 함수는 입력이 음수면 그래디언트가 0이되기 때문에 이런일이 생기면 뉴런이 다시 살아나기 어렵습니다. 이를 해결하기 위해서 LeakyReLU같은 변종을 사용합니다. 이는 LeakyReLU(z)=max(az,z)로 정의 됩니다. a는 함수의 기울기이고 일반적으로 0.01로 설정합니다. 또한 a값을 무작위로 선택하고 테스트시에는 평균을 사용하는 RReLU도 있다. 마지막으로 a가 훈련되는 동안 학습되는 PReLU도 비교했습니다. 이는 대규모 이미지 데이터셋에는 ReLU보다 성능이 크게 앞섰지만 소규모 데이터셋에서는 훈련세트에 과대적합될 위험이 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\matplotlib\\font_manager.py:1316: UserWarning: findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npIISEZFIUkGJiEgkqaBERCSS/h9QwKWLr5wEvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226adaf2c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('pass', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU \", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf에서 LeakyReLU를 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 배치 데이터 정확도: 0.86 검증 세트 정확도: 0.9044\n",
      "5 배치 데이터 정확도: 0.94 검증 세트 정확도: 0.9494\n",
      "10 배치 데이터 정확도: 0.92 검증 세트 정확도: 0.9656\n",
      "15 배치 데이터 정확도: 0.94 검증 세트 정확도: 0.971\n",
      "20 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9762\n",
      "25 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9772\n",
      "30 배치 데이터 정확도: 0.98 검증 세트 정확도: 0.9782\n",
      "35 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9788\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"배치 데이터 정확도:\", acc_batch, \"검증 세트 정확도:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU(exponential linear unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\matplotlib\\font_manager.py:1316: UserWarning: findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226adcbf5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\") #tf.nn.elu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 활성화 함수는 Günter Klambauer, Thomas Unterthiner, Andreas Mayr가 2017년에 쓴 [논문](https://arxiv.org/pdf/1706.02515.pdf)에서 소개되었습니다. 심층 신경망에서 다른 활성화 함수보다 뛰어난 성능을 내므로 꼭 이 함수를 시도해봐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\matplotlib\\font_manager.py:1316: UserWarning: findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhE0v8Hs6bB4DVoN3kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226ade2beb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 활성화 함수를 사용하면 100층으로 된 심층 신경망도 그래디언트 소실/폭주 문제없이 모든 층에서 대략 평균이 0이고 표준 편차가 1을 유지합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "층 0: -0.26 < 평균 < 0.27, 0.74 < 표준 편차 < 1.27\n",
      "층 10: -0.24 < 평균 < 0.27, 0.74 < 표준 편차 < 1.27\n",
      "층 20: -0.17 < 평균 < 0.18, 0.74 < 표준 편차 < 1.24\n",
      "층 30: -0.27 < 평균 < 0.24, 0.78 < 표준 편차 < 1.20\n",
      "층 40: -0.38 < 평균 < 0.39, 0.74 < 표준 편차 < 1.25\n",
      "층 50: -0.27 < 평균 < 0.31, 0.73 < 표준 편차 < 1.27\n",
      "층 60: -0.26 < 평균 < 0.43, 0.74 < 표준 편차 < 1.35\n",
      "층 70: -0.19 < 평균 < 0.21, 0.75 < 표준 편차 < 1.21\n",
      "층 80: -0.18 < 평균 < 0.16, 0.72 < 표준 편차 < 1.19\n",
      "층 90: -0.19 < 평균 < 0.16, 0.75 < 표준 편차 < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"층 {}: {:.2f} < 평균 < {:.2f}, {:.2f} < 표준 편차 < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 텐서플로를 사용한 구현 예입니다(텐서플로 1.4 버전에 `tf.nn.selu()` 함수가 추가되었습니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELU는 드롭아웃과 연결하여 사용할 수 있습니다. Linz 대학교 생물정보학 연구소(Institute of Bioinformatics)의 Johannes Kepler가 만든 [구현](https://github.com/bioinf-jku/SNNs/blob/master/selu.py)을 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELU 활성화 함수를 사용한 신경망을 만들어 MNIST 문제를 풀어 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 훈련할 차례입니다. 입력을 평균 0, 표준 편차 1로 스케일 조정해야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 배치 데이터 정확도: 0.88 검증 세트 정확도: 0.9232\n",
      "5 배치 데이터 정확도: 0.98 검증 세트 정확도: 0.9574\n",
      "10 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9662\n",
      "15 배치 데이터 정확도: 0.96 검증 세트 정확도: 0.9684\n",
      "20 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9692\n",
      "25 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.969\n",
      "30 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9694\n",
      "35 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9702\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"배치 데이터 정확도:\", acc_batch, \"검증 세트 정확도:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서의 ELU와 HE 초기화를 사용하면 훈련 초기 단계에서 그래디언트소실이나 폭주 문제를 크게 감소시킬 수 있지만, 훈련하는 동안 다시 발생하지 않을 수 없습니다. 그렇기 때문에 그래디언트 소실 및 폭주 문제를 해결하기 위해서 배치정규화batch Normalization(BN)을 제안합니다. 이는 훈련하는 동안 이전 층의 파라미터가 변함에 따라 각 층에 들어오는 입력의 분포가 변화되는 문제입니다.(내부 공변량 변화internal Covariate Shift)\n",
    "\n",
    "이 방법은 각 층에서 활성화 함수를 통과하기전에 모델에 연산을 하나 추가합니다. 단순하게 입력데이터의 평균을 0으로 만들고  정규화한 다음, 각 층에서 두 개의 새로운 파라미터로 결과값의 스케일을 조정하고 이동시킵니다.(p.361) \n",
    "\n",
    "테스트 할 떄는 평균과 표준편차를 계산할 미니배치가 없으니 전체 훈련 세트의 평균과 표준편차를 대신 사용합니다. \n",
    "\n",
    "전체적으로 보면 배치 정규화된 층마다 스케일, 이동, 평균, 표준편차 네 개의 파라미터가 학습됩니다. 그러나 단점이라면 층마다 추가되는 계산이 신경망의 예측이 느려지게 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텐서플로 사용해 배치 정규화 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.layers.batch_normalization()사용\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "#훈련하는 동안에는 True, 이외에서는 False\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "#training = training_placeholder, momuntum => v' = v'*momentum + v*(1-momentum) :v= 이동평균(지수감소를 이용한)\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 매개변수를 계속 반복해서 쓰지 않도록 파이썬의 `partial()` 함수를 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 층에 ELU 활성화 함수와 배치 정규화를 사용하여 MNIST를 위한 신경망을 만듭니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노트: 배치 정규화를 위해 별도의 업데이트 연산을 실행해 주어야 합니다(`sess.run([training_op, extra_update_ops],...`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.9042\n",
      "1 검증 세트 정확도: 0.928\n",
      "2 검증 세트 정확도: 0.9374\n",
      "3 검증 세트 정확도: 0.9474\n",
      "4 검증 세트 정확도: 0.9532\n",
      "5 검증 세트 정확도: 0.9572\n",
      "6 검증 세트 정확도: 0.9626\n",
      "7 검증 세트 정확도: 0.9628\n",
      "8 검증 세트 정확도: 0.9664\n",
      "9 검증 세트 정확도: 0.968\n",
      "10 검증 세트 정확도: 0.9694\n",
      "11 검증 세트 정확도: 0.9696\n",
      "12 검증 세트 정확도: 0.971\n",
      "13 검증 세트 정확도: 0.971\n",
      "14 검증 세트 정확도: 0.9728\n",
      "15 검증 세트 정확도: 0.9734\n",
      "16 검증 세트 정확도: 0.9728\n",
      "17 검증 세트 정확도: 0.975\n",
      "18 검증 세트 정확도: 0.9752\n",
      "19 검증 세트 정확도: 0.976\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.batch_normalization()함수에 의존하는 어떤 연산을 수행할 때마다 training 플레이스 홀를 True로 설정\n",
    "2.batch_noramlization()함수는 이동 평균을 갱신하기 위해서 매 훈련 단계에서 평가할 몇개의 연산을 만듭니다. 이 연산은 자동으로 \n",
    "UPDATE_OPS 컬랙션에 추가되므로 이 컬랙션에서 이 연산들을 뽑아내어 훈련이 반복될 때마다 실행시켜주면 된다.\n",
    "'''\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어!? MNIST 정확도가 좋지 않네요. 물론 훈련을 더 오래하면 정확도가 높아지겠지만 이런 얕은 신경망에서는 배치 정규화와 ELU가 큰 효과를 내지 못합니다. 대부분 심층 신경망에서 빛을 발합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "업데이트 연산에 의존하는 훈련 연산을 만들 수도 있습니다:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "이렇게 하면 훈련할 때 `training_op`만 평가하면 텐서플로가 업데이트 연산도 자동으로 실행할 것입니다:\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한가지 더, 훈련될 변수 개수가 전체 전역 변수 개수보다 적습니다. 이동 평균을 위한 변수는 훈련되는 변수가 아니기 때문입니다. 미리 학습한 신경망을 재사용할 경우(아래 참조) 이런 훈련되지 않는 변수를 놓쳐서는 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래디언트 클리핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래디언트 폭주문제를 줄이는 쉬운방법은 역전파될 때 일정 임계값을 넘어서지 못하게 그래디언트를 그냥 단순하게 잘라내는 것입니다. 일반적으로는 배치정규화를 선호하지만 이러한 방법도 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 옵티마이저의 minimize()함수는 그래디언트 계산과 적용 두가지를 처리합니다. 그래서 compute_gradient() clip_by_value() apply_gradients()가 필요합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 그래디언트 클리핑을 적용합니다. 먼저 그래디언트를 구한 다음 `clip_by_value()` 함수를 사용해 클리핑하고 적용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#gradient값과 variable을 결과값으로 내보냄\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "# Clips tensor values to a specified min and max.\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "#optimizer에 적용(minimize가 적용되 있다.)\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.2878\n",
      "1 검증 세트 정확도: 0.7942\n",
      "2 검증 세트 정확도: 0.8794\n",
      "3 검증 세트 정확도: 0.9054\n",
      "4 검증 세트 정확도: 0.9162\n",
      "5 검증 세트 정확도: 0.9212\n",
      "6 검증 세트 정확도: 0.9292\n",
      "7 검증 세트 정확도: 0.9358\n",
      "8 검증 세트 정확도: 0.9378\n",
      "9 검증 세트 정확도: 0.9414\n",
      "10 검증 세트 정확도: 0.9456\n",
      "11 검증 세트 정확도: 0.9472\n",
      "12 검증 세트 정확도: 0.9476\n",
      "13 검증 세트 정확도: 0.9536\n",
      "14 검증 세트 정확도: 0.9564\n",
      "15 검증 세트 정확도: 0.9566\n",
      "16 검증 세트 정확도: 0.9574\n",
      "17 검증 세트 정확도: 0.9588\n",
      "18 검증 세트 정확도: 0.9624\n",
      "19 검증 세트 정확도: 0.9616\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 미리 훈련된 층 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로 모델 재사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 그래프 구조를 로드해야 합니다. `import_meta_graph()` 함수가 그래프 연산들을 로드하여 기본 그래프에 적재하고 모델의 상태를 복원할 수 있도록 `Saver` 객체를 반환합니다. 기본적으로 `Saver` 객체는 `.meta` 확장자를 가진 파일에 그래프 구조를 저장하므로 이 파일을 로드해야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 훈련해야될 모든 연산과 텐서를 직접 지정해야 합니다. 그래프 구조를 모를때 모든 연산을 출력해서 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2/tensor_names\n",
      "save/Const\n",
      "save/RestoreV2\n",
      "eval/Const\n",
      "eval/in_top_k/InTopKV2/k\n",
      "GradientDescent/learning_rate\n",
      "clip_by_value_11/clip_value_max\n",
      "clip_by_value_11/clip_value_min\n",
      "clip_by_value_10/clip_value_max\n",
      "clip_by_value_10/clip_value_min\n",
      "clip_by_value_9/clip_value_max\n",
      "clip_by_value_9/clip_value_min\n",
      "clip_by_value_8/clip_value_max\n",
      "clip_by_value_8/clip_value_min\n",
      "clip_by_value_7/clip_value_max\n",
      "clip_by_value_7/clip_value_min\n",
      "clip_by_value_6/clip_value_max\n",
      "clip_by_value_6/clip_value_min\n",
      "clip_by_value_5/clip_value_max\n",
      "clip_by_value_5/clip_value_min\n",
      "clip_by_value_4/clip_value_max\n",
      "clip_by_value_4/clip_value_min\n",
      "clip_by_value_3/clip_value_max\n",
      "clip_by_value_3/clip_value_min\n",
      "clip_by_value_2/clip_value_max\n",
      "clip_by_value_2/clip_value_min\n",
      "clip_by_value_1/clip_value_max\n",
      "clip_by_value_1/clip_value_min\n",
      "clip_by_value/clip_value_max\n",
      "clip_by_value/clip_value_min\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Shape\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape\n",
      "loss/Const\n",
      "outputs/bias\n",
      "save/Assign_10\n",
      "outputs/bias/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias/Assign\n",
      "outputs/kernel\n",
      "save/Assign_11\n",
      "outputs/kernel/read\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel/Assign\n",
      "hidden5/bias\n",
      "save/Assign_8\n",
      "hidden5/bias/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias/Assign\n",
      "hidden5/kernel\n",
      "save/Assign_9\n",
      "hidden5/kernel/read\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel/Assign\n",
      "hidden4/bias\n",
      "save/Assign_6\n",
      "hidden4/bias/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias/Assign\n",
      "hidden4/kernel\n",
      "save/Assign_7\n",
      "hidden4/kernel/read\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel/Assign\n",
      "hidden3/bias\n",
      "save/Assign_4\n",
      "hidden3/bias/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias/Assign\n",
      "hidden3/kernel\n",
      "save/Assign_5\n",
      "hidden3/kernel/read\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel/Assign\n",
      "hidden2/bias\n",
      "save/Assign_2\n",
      "hidden2/bias/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias/Assign\n",
      "hidden2/kernel\n",
      "save/Assign_3\n",
      "hidden2/kernel/read\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel/Assign\n",
      "hidden1/bias\n",
      "save/Assign\n",
      "hidden1/bias/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias/Assign\n",
      "hidden1/kernel\n",
      "save/Assign_1\n",
      "save/restore_all\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "hidden1/kernel/read\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel/Assign\n",
      "init\n",
      "y\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "X\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/accuracy\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/zeros_like\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_11\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_10\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_9\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_8\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_7\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_6\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_5\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_4\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_3\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_2\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_1\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "loss/loss\n",
      "save/RestoreV2/shape_and_slices_1\n",
      "save/RestoreV2/tensor_names_1\n",
      "save/SaveV2/shape_and_slices_1\n",
      "save/SaveV2/tensor_names_1\n",
      "save/Const_1\n",
      "save/RestoreV2_1\n",
      "eval/Const_1\n",
      "eval/in_top_k/InTopKV2/k_1\n",
      "GradientDescent/learning_rate_1\n",
      "clip_by_value_11/clip_value_max_1\n",
      "clip_by_value_11/clip_value_min_1\n",
      "clip_by_value_10/clip_value_max_1\n",
      "clip_by_value_10/clip_value_min_1\n",
      "clip_by_value_9/clip_value_max_1\n",
      "clip_by_value_9/clip_value_min_1\n",
      "clip_by_value_8/clip_value_max_1\n",
      "clip_by_value_8/clip_value_min_1\n",
      "clip_by_value_7/clip_value_max_1\n",
      "clip_by_value_7/clip_value_min_1\n",
      "clip_by_value_6/clip_value_max_1\n",
      "clip_by_value_6/clip_value_min_1\n",
      "clip_by_value_5/clip_value_max_1\n",
      "clip_by_value_5/clip_value_min_1\n",
      "clip_by_value_4/clip_value_max_1\n",
      "clip_by_value_4/clip_value_min_1\n",
      "clip_by_value_3/clip_value_max_1\n",
      "clip_by_value_3/clip_value_min_1\n",
      "clip_by_value_2/clip_value_max_1\n",
      "clip_by_value_2/clip_value_min_1\n",
      "clip_by_value_1/clip_value_max_1\n",
      "clip_by_value_1/clip_value_min_1\n",
      "clip_by_value/clip_value_max_1\n",
      "clip_by_value/clip_value_min_1\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim_1\n",
      "gradients/loss/loss_grad/Maximum/y_1\n",
      "gradients/loss/loss_grad/Const_1_1\n",
      "gradients/loss/loss_grad/Const_2\n",
      "gradients/loss/loss_grad/Shape_2_1\n",
      "gradients/loss/loss_grad/Prod_1_1\n",
      "gradients/loss/loss_grad/Maximum_1\n",
      "gradients/loss/loss_grad/Reshape/shape_1\n",
      "gradients/grad_ys_0_1\n",
      "gradients/Shape_1\n",
      "gradients/Fill_1\n",
      "gradients/loss/loss_grad/Reshape_1\n",
      "loss/Const_1\n",
      "outputs/bias_1\n",
      "save/Assign_10_1\n",
      "outputs/bias/read_1\n",
      "outputs/bias/Initializer/zeros_1\n",
      "outputs/bias/Assign_1\n",
      "outputs/kernel_1\n",
      "save/Assign_11_1\n",
      "outputs/kernel/read_1\n",
      "outputs/kernel/Initializer/random_uniform/max_1\n",
      "outputs/kernel/Initializer/random_uniform/min_1\n",
      "outputs/kernel/Initializer/random_uniform/sub_1\n",
      "outputs/kernel/Initializer/random_uniform/shape_1\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform_1\n",
      "outputs/kernel/Initializer/random_uniform/mul_1\n",
      "outputs/kernel/Initializer/random_uniform_1\n",
      "outputs/kernel/Assign_1\n",
      "hidden5/bias_1\n",
      "save/Assign_8_1\n",
      "hidden5/bias/read_1\n",
      "hidden5/bias/Initializer/zeros_1\n",
      "hidden5/bias/Assign_1\n",
      "hidden5/kernel_1\n",
      "save/Assign_9_1\n",
      "hidden5/kernel/read_1\n",
      "hidden5/kernel/Initializer/random_uniform/max_1\n",
      "hidden5/kernel/Initializer/random_uniform/min_1\n",
      "hidden5/kernel/Initializer/random_uniform/sub_1\n",
      "hidden5/kernel/Initializer/random_uniform/shape_1\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform_1\n",
      "hidden5/kernel/Initializer/random_uniform/mul_1\n",
      "hidden5/kernel/Initializer/random_uniform_1\n",
      "hidden5/kernel/Assign_1\n",
      "hidden4/bias_1\n",
      "save/Assign_6_1\n",
      "hidden4/bias/read_1\n",
      "hidden4/bias/Initializer/zeros_1\n",
      "hidden4/bias/Assign_1\n",
      "hidden4/kernel_1\n",
      "save/Assign_7_1\n",
      "hidden4/kernel/read_1\n",
      "hidden4/kernel/Initializer/random_uniform/max_1\n",
      "hidden4/kernel/Initializer/random_uniform/min_1\n",
      "hidden4/kernel/Initializer/random_uniform/sub_1\n",
      "hidden4/kernel/Initializer/random_uniform/shape_1\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform_1\n",
      "hidden4/kernel/Initializer/random_uniform/mul_1\n",
      "hidden4/kernel/Initializer/random_uniform_1\n",
      "hidden4/kernel/Assign_1\n",
      "hidden3/bias_1\n",
      "save/Assign_4_1\n",
      "hidden3/bias/read_1\n",
      "hidden3/bias/Initializer/zeros_1\n",
      "hidden3/bias/Assign_1\n",
      "hidden3/kernel_1\n",
      "save/Assign_5_1\n",
      "hidden3/kernel/read_1\n",
      "hidden3/kernel/Initializer/random_uniform/max_1\n",
      "hidden3/kernel/Initializer/random_uniform/min_1\n",
      "hidden3/kernel/Initializer/random_uniform/sub_1\n",
      "hidden3/kernel/Initializer/random_uniform/shape_1\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform_1\n",
      "hidden3/kernel/Initializer/random_uniform/mul_1\n",
      "hidden3/kernel/Initializer/random_uniform_1\n",
      "hidden3/kernel/Assign_1\n",
      "hidden2/bias_1\n",
      "save/Assign_2_1\n",
      "hidden2/bias/read_1\n",
      "hidden2/bias/Initializer/zeros_1\n",
      "hidden2/bias/Assign_1\n",
      "hidden2/kernel_1\n",
      "save/Assign_3_1\n",
      "hidden2/kernel/read_1\n",
      "hidden2/kernel/Initializer/random_uniform/max_1\n",
      "hidden2/kernel/Initializer/random_uniform/min_1\n",
      "hidden2/kernel/Initializer/random_uniform/sub_1\n",
      "hidden2/kernel/Initializer/random_uniform/shape_1\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform_1\n",
      "hidden2/kernel/Initializer/random_uniform/mul_1\n",
      "hidden2/kernel/Initializer/random_uniform_1\n",
      "hidden2/kernel/Assign_1\n",
      "hidden1/bias_1\n",
      "save/Assign_12\n",
      "hidden1/bias/read_1\n",
      "hidden1/bias/Initializer/zeros_1\n",
      "hidden1/bias/Assign_1\n",
      "hidden1/kernel_1\n",
      "save/Assign_1_1\n",
      "save/restore_all_1\n",
      "save/SaveV2_1\n",
      "save/control_dependency_1\n",
      "hidden1/kernel/read_1\n",
      "hidden1/kernel/Initializer/random_uniform/max_1\n",
      "hidden1/kernel/Initializer/random_uniform/min_1\n",
      "hidden1/kernel/Initializer/random_uniform/sub_1\n",
      "hidden1/kernel/Initializer/random_uniform/shape_1\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform_1\n",
      "hidden1/kernel/Initializer/random_uniform/mul_1\n",
      "hidden1/kernel/Initializer/random_uniform_1\n",
      "hidden1/kernel/Assign_1\n",
      "init_1\n",
      "y_1\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape_1\n",
      "X_1\n",
      "dnn/hidden1/MatMul_1\n",
      "dnn/hidden1/BiasAdd_1\n",
      "dnn/hidden1/Relu_1\n",
      "dnn/hidden2/MatMul_1\n",
      "dnn/hidden2/BiasAdd_1\n",
      "dnn/hidden2/Relu_1\n",
      "dnn/hidden3/MatMul_1\n",
      "dnn/hidden3/BiasAdd_1\n",
      "dnn/hidden3/Relu_1\n",
      "dnn/hidden4/MatMul_1\n",
      "dnn/hidden4/BiasAdd_1\n",
      "dnn/hidden4/Relu_1\n",
      "dnn/hidden5/MatMul_1\n",
      "dnn/hidden5/BiasAdd_1\n",
      "dnn/hidden5/Relu_1\n",
      "dnn/outputs/MatMul_1\n",
      "dnn/outputs/BiasAdd_1\n",
      "eval/in_top_k/InTopKV2_1\n",
      "eval/Cast_1\n",
      "eval/accuracy_1\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_1\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient_1\n",
      "gradients/zeros_like_1\n",
      "gradients/loss/loss_grad/Shape_1_1\n",
      "gradients/loss/loss_grad/Prod_2\n",
      "gradients/loss/loss_grad/floordiv_1\n",
      "gradients/loss/loss_grad/Cast_1\n",
      "gradients/loss/loss_grad/Shape_3\n",
      "gradients/loss/loss_grad/Tile_1\n",
      "gradients/loss/loss_grad/truediv_1\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims_1\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul_1\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad_1\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps_1\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_11_1\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent_1\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_2\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_2\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_10_1\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad_1\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad_1\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_9_1\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_2\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_8_1\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad_1\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad_1\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_7_1\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_2\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_6_1\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad_1\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad_1\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_5_1\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_2\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_4_1\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad_1\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad_1\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_3_1\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_2\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_2_1\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad_1\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad_1\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_1_1\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent_1\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_2\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_2\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1_1\n",
      "clip_by_value_12\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent_1\n",
      "GradientDescent_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_2\n",
      "loss/loss_1\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 연산을 찾았다면 그래프의 `get_operation_by_name()`이나 `get_tensor_by_name()` 메서드를 사용하여 추출할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 모델을 만들 때 다른 사람이 재사용하기 쉽게 연산에 명확한 이름을 부여하고 문서화를 하는 것이 좋습니다. 또 다른 방법은 처리해야 할 중요한 연산들을 모두 모아 놓은 컬렉션을 만드는 것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 해서 모델을 재사용할 때 다음과 같이 간단하게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 세션을 시작해서 모델을 복원하고 준비된 훈련데이터로 훈련을 할수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.9642\n",
      "1 검증 세트 정확도: 0.9628\n",
      "2 검증 세트 정확도: 0.9654\n",
      "3 검증 세트 정확도: 0.9652\n",
      "4 검증 세트 정확도: 0.9642\n",
      "5 검증 세트 정확도: 0.9648\n",
      "6 검증 세트 정확도: 0.969\n",
      "7 검증 세트 정확도: 0.9686\n",
      "8 검증 세트 정확도: 0.9684\n",
      "9 검증 세트 정확도: 0.9684\n",
      "10 검증 세트 정확도: 0.9704\n",
      "11 검증 세트 정확도: 0.9714\n",
      "12 검증 세트 정확도: 0.9672\n",
      "13 검증 세트 정확도: 0.9702\n",
      "14 검증 세트 정확도: 0.9712\n",
      "15 검증 세트 정확도: 0.9724\n",
      "16 검증 세트 정확도: 0.972\n",
      "17 검증 세트 정확도: 0.971\n",
      "18 검증 세트 정확도: 0.9714\n",
      "19 검증 세트 정확도: 0.9714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러오기위한 다른 방법은 원본 그래프를 만든 파이썬 코드에 접근가능하다면 그대로 복사해서 사용하면됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.9642\n",
      "1 검증 세트 정확도: 0.9628\n",
      "2 검증 세트 정확도: 0.9654\n",
      "3 검증 세트 정확도: 0.9652\n",
      "4 검증 세트 정확도: 0.9642\n",
      "5 검증 세트 정확도: 0.9648\n",
      "6 검증 세트 정확도: 0.969\n",
      "7 검증 세트 정확도: 0.9686\n",
      "8 검증 세트 정확도: 0.9684\n",
      "9 검증 세트 정확도: 0.9684\n",
      "10 검증 세트 정확도: 0.9704\n",
      "11 검증 세트 정확도: 0.9714\n",
      "12 검증 세트 정확도: 0.9672\n",
      "13 검증 세트 정확도: 0.9702\n",
      "14 검증 세트 정확도: 0.9712\n",
      "15 검증 세트 정확도: 0.9724\n",
      "16 검증 세트 정확도: 0.972\n",
      "17 검증 세트 정확도: 0.971\n",
      "18 검증 세트 정확도: 0.9714\n",
      "19 검증 세트 정확도: 0.9714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 하위층만 재사용할 것입니다. `import_meta_graph()`를 사용하면 전체 그래프를 로드하지만 필요하지 않은 부분은 무시하면 됩니다. 이 예에서는 학습된 3번째 층 위에 4번째 은닉층을 새로 추가합니다(원래 4번째 층은 무시됩니다). 새로운 출력층도 추가하고 이 출력으로 손실을 계산하고 이를 최소화하기 위한 새로운 옵티마이저를 만듭니다. 전체 그래프(원본 그래프 전체와 새로운 연산)를 저장할 새로운 `Saver` 객체와 새로운 모든 변수를 초기화할 초기화 연산도 필요합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # 새 층\n",
    "n_outputs = 10  # 새 층\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.923\n",
      "1 검증 세트 정확도: 0.945\n",
      "2 검증 세트 정확도: 0.9534\n",
      "3 검증 세트 정확도: 0.958\n",
      "4 검증 세트 정확도: 0.9608\n",
      "5 검증 세트 정확도: 0.9562\n",
      "6 검증 세트 정확도: 0.9624\n",
      "7 검증 세트 정확도: 0.9626\n",
      "8 검증 세트 정확도: 0.9642\n",
      "9 검증 세트 정확도: 0.9644\n",
      "10 검증 세트 정확도: 0.966\n",
      "11 검증 세트 정확도: 0.9666\n",
      "12 검증 세트 정확도: 0.9648\n",
      "13 검증 세트 정확도: 0.968\n",
      "14 검증 세트 정확도: 0.9676\n",
      "15 검증 세트 정확도: 0.969\n",
      "16 검증 세트 정확도: 0.9692\n",
      "17 검증 세트 정확도: 0.9696\n",
      "18 검증 세트 정확도: 0.9684\n",
      "19 검증 세트 정확도: 0.9674\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 모델을 만든 파이썬 코드에 접근할 수 있다면 필요한 부분만 재사용하고 나머지는 버릴 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # 재사용\n",
    "n_hidden2 = 50  # 재사용\n",
    "n_hidden3 = 50  # 재사용\n",
    "n_hidden4 = 20  # 새로 만듦!\n",
    "n_outputs = 10  # 새로 만듦!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # 재사용\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # 재사용\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # 재사용\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # 새로 만듦!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # 새로 만듦!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 이전에 학습된 모델을 복원하기 위해 (복원할 변수 리스트를 전달합니다. 그렇지 않으면 그래프와 맞지 않는다고 에러를 낼 것입니다) `Saver` 객체를 하나 만들고 훈련이 끝난 후 새로운 모델을 저장하기 위해 또 다른 `Saver` 객체를 만들어야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.9018\n",
      "1 검증 세트 정확도: 0.9324\n",
      "2 검증 세트 정확도: 0.9432\n",
      "3 검증 세트 정확도: 0.947\n",
      "4 검증 세트 정확도: 0.9516\n",
      "5 검증 세트 정확도: 0.9532\n",
      "6 검증 세트 정확도: 0.956\n",
      "7 검증 세트 정확도: 0.959\n",
      "8 검증 세트 정확도: 0.9586\n",
      "9 검증 세트 정확도: 0.9612\n",
      "10 검증 세트 정확도: 0.9628\n",
      "11 검증 세트 정확도: 0.9618\n",
      "12 검증 세트 정확도: 0.9638\n",
      "13 검증 세트 정확도: 0.9658\n",
      "14 검증 세트 정확도: 0.9664\n",
      "15 검증 세트 정확도: 0.9662\n",
      "16 검증 세트 정확도: 0.9674\n",
      "17 검증 세트 정확도: 0.9676\n",
      "18 검증 세트 정확도: 0.968\n",
      "19 검증 세트 정확도: 0.9674\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # 정규표현식 변수가져오기 \n",
    "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\") #위에 해당하는 변수 불러오기\n",
    "\n",
    "    for epoch in range(n_epochs):                                        \n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})    \n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)                    \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tf.get_default_graph().get_tensor_by_name()\n",
    "2. tf.get_default_graph().get_operation_by_name()\n",
    "3. tf.get_collection()\n",
    "\n",
    "variable\n",
    "\n",
    "변수 v를 만들면 변수값을 읽고(v/read) 초기값(v/initial_value)을 할당(v/Assign)하는 노드가 각각 그래프에 추가된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 프레임워크의 모델 재사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 예에서는 재사용하려는 각 변수에 대해 변수 초기화 할당 연산을 찾고, 초기화 될 값에 해당하는 두 번째 입력 핸들을 구합니다. 초기화가 실행될 때 여기에 `feed_dict` 매개변수를 사용하여 초깃값 대신 원하는 값을 주입합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # 다른 프레임워크로부터 가중치를 로드\n",
    "original_b = [7., 8., 9.]                 # 다른 프레임워크로부터 편향을 로드\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] 모델의 나머지 부분을 구성\n",
    "\n",
    "# hidden1 변수의 할당 노드에 대한 핸들을 구합니다\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1] #0은 v변수자체 1은 할달될값(v/inital_value)\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] 새 작업에 모델을 훈련시킵니다\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # 책에는 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 방법은 전용 할당 노드와 플레이스홀더를 만든는 것입니다. 이 방법은 더 번거롭고 효율적이지 않지만 하려는 방식이 잘 드러나는 방법입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # 다른 프레임워크로부터 가중치를 로드\n",
    "original_b = [7., 8., 9.]                 # 다른 프레임워크로부터 편향을 로드\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] 모델의 나머지를 구성\n",
    "\n",
    "# hidden1 변수의 할당 노드에 대한 핸들을 구합니다\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # 루트 범위\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# 전용 플레이스홀더와 할당 노드를 만듭니다\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] 새 작업에 모델을 훈련시킵니다\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_collection()`에 `scope`를 지정하여 변수의 핸들을 가져올 수도 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 그래프의 `get_tensor_by_name()` 메서드를 사용할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하위층 동결하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN의 하위층은 이미지에 있는 저수준 특성을 감지하도록 되있어서 이층들을 그대로 재사용 하고 싶을 때 하는 방법이 이 층의 가중치를 '동결' 시키는 것입니다. 이러면 상위층의 가중치를 훈련시키기 쉬워집니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 하위층의 변수를 제외하고 훈련시킬 변수 목록을 옵티마이저에 전달 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # 재사용\n",
    "n_hidden2 = 50  # 재사용\n",
    "n_hidden3 = 50  # 재사용\n",
    "n_hidden4 = 20  # 새로 만듦!\n",
    "n_outputs = 10  # 새로 만듦!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # 재사용\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # 재사용\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # 재사용\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # 새로 만듦!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # 새로 만듦!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var_list로 학습할 변수 목록을 제공하는 방법\n",
    "#scope='hidden[34]|outputs'=>3,4층의 출력층에 있는 학습할 변수목록을 모두구함\n",
    "with tf.name_scope(\"train\"):                                         \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     \n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.896\n",
      "1 검증 세트 정확도: 0.9304\n",
      "2 검증 세트 정확도: 0.9398\n",
      "3 검증 세트 정확도: 0.9446\n",
      "4 검증 세트 정확도: 0.9482\n",
      "5 검증 세트 정확도: 0.9504\n",
      "6 검증 세트 정확도: 0.9506\n",
      "7 검증 세트 정확도: 0.9536\n",
      "8 검증 세트 정확도: 0.9552\n",
      "9 검증 세트 정확도: 0.9564\n",
      "10 검증 세트 정확도: 0.956\n",
      "11 검증 세트 정확도: 0.9566\n",
      "12 검증 세트 정확도: 0.9568\n",
      "13 검증 세트 정확도: 0.9572\n",
      "14 검증 세트 정확도: 0.959\n",
      "15 검증 세트 정확도: 0.9576\n",
      "16 검증 세트 정확도: 0.9578\n",
      "17 검증 세트 정확도: 0.9598\n",
      "18 검증 세트 정확도: 0.959\n",
      "19 검증 세트 정확도: 0.9598\n"
     ]
    }
   ],
   "source": [
    "#123층의 계산된 변수만을 불러온다.\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # 정규 표현식\n",
    "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.stop_gradient()층을 추가하는 것 이렇게 하면 이 층 아래의 모든 층이 고정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # 재사용\n",
    "n_hidden2 = 50  # 재사용\n",
    "n_hidden3 = 50  # 재사용\n",
    "n_hidden4 = 20  # 새로 만듦!\n",
    "n_outputs = 10  # 새로 만듦!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # 동결층 재사용\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # 동결층 재사용\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # 동결하지 않고 재사용\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # 새로 만듦!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # 새로 만듦!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.9022\n",
      "1 검증 세트 정확도: 0.9306\n",
      "2 검증 세트 정확도: 0.9434\n",
      "3 검증 세트 정확도: 0.9474\n",
      "4 검증 세트 정확도: 0.9514\n",
      "5 검증 세트 정확도: 0.9522\n",
      "6 검증 세트 정확도: 0.9524\n",
      "7 검증 세트 정확도: 0.9556\n",
      "8 검증 세트 정확도: 0.9554\n",
      "9 검증 세트 정확도: 0.956\n",
      "10 검증 세트 정확도: 0.9572\n",
      "11 검증 세트 정확도: 0.955\n",
      "12 검증 세트 정확도: 0.9574\n",
      "13 검증 세트 정확도: 0.958\n",
      "14 검증 세트 정확도: 0.9582\n",
      "15 검증 세트 정확도: 0.958\n",
      "16 검증 세트 정확도: 0.9566\n",
      "17 검증 세트 정확도: 0.9576\n",
      "18 검증 세트 정확도: 0.9592\n",
      "19 검증 세트 정확도: 0.9588\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # 정규 표현식\n",
    "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동결된층 캐싱하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2번째층까지 동결되있기 때문에 앞서 처음부터 미니배치를 통해서 뿌려주기보다 한번에 2번째 층까지의 출력을 받고 그 출력을 미니배치하여 주입하면 속도가 빨라진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b1fbf35cdc41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mtraining_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # 재사용\n",
    "n_hidden2 = 50  # 재사용\n",
    "n_hidden3 = 50  # 재사용\n",
    "n_hidden4 = 20  # 새로 만듦!\n",
    "n_outputs = 10  # 새로 만듦!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # 동결층 재사용\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # 동결층 재사용 & 캐싱\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # 동결하지 않고 재사용\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # 새로 만듦!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # 새로 만듦!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # 정규 표현식\n",
    "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.9022\n",
      "1 검증 세트 정확도: 0.9306\n",
      "2 검증 세트 정확도: 0.9434\n",
      "3 검증 세트 정확도: 0.9474\n",
      "4 검증 세트 정확도: 0.9514\n",
      "5 검증 세트 정확도: 0.9522\n",
      "6 검증 세트 정확도: 0.9524\n",
      "7 검증 세트 정확도: 0.9556\n",
      "8 검증 세트 정확도: 0.9554\n",
      "9 검증 세트 정확도: 0.956\n",
      "10 검증 세트 정확도: 0.9572\n",
      "11 검증 세트 정확도: 0.955\n",
      "12 검증 세트 정확도: 0.9574\n",
      "13 검증 세트 정확도: 0.958\n",
      "14 검증 세트 정확도: 0.9582\n",
      "15 검증 세트 정확도: 0.958\n",
      "16 검증 세트 정확도: 0.9566\n",
      "17 검증 세트 정확도: 0.9576\n",
      "18 검증 세트 정확도: 0.9592\n",
      "19 검증 세트 정확도: 0.9588\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid})\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, \n",
    "                                                y: y_valid})             \n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)            \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  고속 옵티마이져"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모멘텀 옵티"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네스테로프 가속 경사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 스케줄링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):       \n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.9574\n",
      "1 검증 세트 정확도: 0.9716\n",
      "2 검증 세트 정확도: 0.973\n",
      "3 검증 세트 정확도: 0.9798\n",
      "4 검증 세트 정확도: 0.9816\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 규제(Regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$과 $\\ell_2$ 규제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1규제에 대한 기본적인 방법으로 먼저 평상시의 모델을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 규제를 가할 곳을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_gㅡph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 규제 하이퍼파라미터\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.831\n",
      "1 검증 세트 정확도: 0.871\n",
      "2 검증 세트 정확도: 0.8838\n",
      "3 검증 세트 정확도: 0.8934\n",
      "4 검증 세트 정확도: 0.8966\n",
      "5 검증 세트 정확도: 0.8988\n",
      "6 검증 세트 정확도: 0.9016\n",
      "7 검증 세트 정확도: 0.9044\n",
      "8 검증 세트 정확도: 0.9058\n",
      "9 검증 세트 정확도: 0.906\n",
      "10 검증 세트 정확도: 0.9068\n",
      "11 검증 세트 정확도: 0.9054\n",
      "12 검증 세트 정확도: 0.907\n",
      "13 검증 세트 정확도: 0.9084\n",
      "14 검증 세트 정확도: 0.9088\n",
      "15 검증 세트 정확도: 0.9064\n",
      "16 검증 세트 정확도: 0.9068\n",
      "17 검증 세트 정확도: 0.9066\n",
      "18 검증 세트 정확도: 0.9066\n",
      "19 검증 세트 정확도: 0.9052\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 방법으로 변수를 생성하는 함수(get_variable, tf.layer.dense)는 각 변수에 대한 매개변수(*_regularizer)를 제공합니다. 그리고 가중치를 받아서 규제에 맞는 손실을 반환하는 함수도 이 매개변수에 전달할 수 있습니다. 이는 l1_regularizer(), l2_regularizer(), l1_l2_regularizer() 함수가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale)) \n",
    "#kernel_regularizer를 통해서 규제에 대한 함수를 받아쓴다. \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     \n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  \n",
    "        labels=y, logits=logits)                                \n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\") \n",
    "    #규제손실을 포함하고 있는 특별한 컬렉션에서 규제손실을 가져온다.(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.8274\n",
      "1 검증 세트 정확도: 0.8766\n",
      "2 검증 세트 정확도: 0.8952\n",
      "3 검증 세트 정확도: 0.9016\n",
      "4 검증 세트 정확도: 0.908\n",
      "5 검증 세트 정확도: 0.9096\n",
      "6 검증 세트 정확도: 0.9124\n",
      "7 검증 세트 정확도: 0.9154\n",
      "8 검증 세트 정확도: 0.9178\n",
      "9 검증 세트 정확도: 0.919\n",
      "10 검증 세트 정확도: 0.92\n",
      "11 검증 세트 정확도: 0.9224\n",
      "12 검증 세트 정확도: 0.9212\n",
      "13 검증 세트 정확도: 0.9228\n",
      "14 검증 세트 정확도: 0.9222\n",
      "15 검증 세트 정확도: 0.9218\n",
      "16 검증 세트 정확도: 0.9218\n",
      "17 검증 세트 정확도: 0.9228\n",
      "18 검증 세트 정확도: 0.9216\n",
      "19 검증 세트 정확도: 0.9214\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 드롭아웃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.9258\n",
      "1 검증 세트 정확도: 0.945\n",
      "2 검증 세트 정확도: 0.9488\n",
      "3 검증 세트 정확도: 0.9576\n",
      "4 검증 세트 정확도: 0.9612\n",
      "5 검증 세트 정확도: 0.9618\n",
      "6 검증 세트 정확도: 0.962\n",
      "7 검증 세트 정확도: 0.9668\n",
      "8 검증 세트 정확도: 0.968\n",
      "9 검증 세트 정확도: 0.97\n",
      "10 검증 세트 정확도: 0.9692\n",
      "11 검증 세트 정확도: 0.9698\n",
      "12 검증 세트 정확도: 0.9712\n",
      "13 검증 세트 정확도: 0.9718\n",
      "14 검증 세트 정확도: 0.9712\n",
      "15 검증 세트 정확도: 0.9692\n",
      "16 검증 세트 정확도: 0.971\n",
      "17 검증 세트 정확도: 0.9732\n",
      "18 검증 세트 정확도: 0.9724\n",
      "19 검증 세트 정확도: 0.9726\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맥스-노름 규제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 2개층을 가진 mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치에 대한 핸들을 얻고 `clip_by_norm()` 함수를 사용해 가중치를 클리핑하는 연산을 만듭니다. 그런 다음 클리핑된 가중치를 가중치 변수에 할당하는 연산을 만듭니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 훈련시킵니다. 이전과 매우 동일한데 `training_op`을 실행한 후에 `clip_weights`와 `clip_weights2` 연산을 실행하는 것만 다릅니다:\n",
    "\n",
    "훈련 한번 할 때마다 가중지에 대해서 맥스노름을 실행시켜주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.9568\n",
      "1 검증 세트 정확도: 0.97\n",
      "2 검증 세트 정확도: 0.9708\n",
      "3 검증 세트 정확도: 0.9774\n",
      "4 검증 세트 정확도: 0.977\n",
      "5 검증 세트 정확도: 0.9786\n",
      "6 검증 세트 정확도: 0.982\n",
      "7 검증 세트 정확도: 0.9822\n",
      "8 검증 세트 정확도: 0.9802\n",
      "9 검증 세트 정확도: 0.9818\n",
      "10 검증 세트 정확도: 0.9828\n",
      "11 검증 세트 정확도: 0.9846\n",
      "12 검증 세트 정확도: 0.9828\n",
      "13 검증 세트 정확도: 0.9838\n",
      "14 검증 세트 정확도: 0.9844\n",
      "15 검증 세트 정확도: 0.9848\n",
      "16 검증 세트 정확도: 0.9842\n",
      "17 검증 세트 정확도: 0.984\n",
      "18 검증 세트 정확도: 0.9848\n",
      "19 검증 세트 정확도: 0.985\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:                                              \n",
    "    init.run()                                                          \n",
    "    for epoch in range(n_epochs):                                       \n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):  \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        \n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)                    \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이보다 더 좋은 방법은 max_norm_regularizer()함수는 만드는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # 규제 손실을 위한 항이 없습니다\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음 (필요한 임계값을 지정해서) 맥스 노름 규제 매개변수에 넘길 함수를 만들기 위해 이 함수를 호출합니다. 은닉층을 만들 때 이 규제 함수를 `kernel_regularizer` 매개변수를 통해 전달할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8f0b91aac1c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclip_all_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_norm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)                    \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
